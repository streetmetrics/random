{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/20 21:26:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lit, to_timestamp, current_timestamp, struct, to_date, from_unixtime\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, StructType, StructField, DateType\n",
    "import h3\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "MARKETS = ['9D37-NEW-3259','CCC2-BOS-25AA']\n",
    "PREFIX = \"2024/01/01/\"\n",
    "\n",
    "# MARKETS = None\n",
    "# PREFIX = None\n",
    "\n",
    "H3_RESOLUTION_BASE = 15\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/sunaybhat/Documents/spark-3.5.4-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/sunaybhat/miniconda3/envs/s3_h3_env/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/Users/sunaybhat/miniconda3/envs/s3_h3_env/bin/python\"\n",
    "\n",
    "CONFIG_PATH = '/Users/sunaybhat/Documents/GitHub/DS_config.yaml'\n",
    "with open(CONFIG_PATH, 'r') as yamlfile: config = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 H3 Indexation\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", config['AWS']['KEYs']['Access']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", config['AWS']['KEYs']['Secret']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# S3 Configuration\n",
    "SOURCE_BUCKET = \"near-market-data\"\n",
    "TARGET_BUCKET = \"azira-backfill-snowflake\"\n",
    "\n",
    "S3_CLIENT = boto3.client(\n",
    "        's3',\n",
    "        region_name=config['AWS']['Region'],\n",
    "        aws_access_key_id=config['AWS']['KEYs']['Access'],\n",
    "        aws_secret_access_key=config['AWS']['KEYs']['Secret']    )\n",
    "\n",
    "\n",
    "# Define schema based on Snowflake procedure\n",
    "schema = StructType([\n",
    "    StructField(\"device_id\", StringType()),\n",
    "    StructField(\"unix_ts\", IntegerType()),\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"lon\", DoubleType()),\n",
    "    StructField(\"datasource_id\", IntegerType()),\n",
    "    StructField(\"accuracy\", IntegerType()),\n",
    "    StructField(\"device_ip\", StringType()),\n",
    "    StructField(\"user_agent\", StringType()),\n",
    "    StructField(\"publisher_id\", IntegerType()),\n",
    "    StructField(\"make\", StringType()),\n",
    "    StructField(\"os\", StringType()),\n",
    "    StructField(\"os_version\", StringType()),\n",
    "    StructField(\"categories\", StringType()),\n",
    "    StructField(\"country_code\", StringType()),\n",
    "    StructField(\"market_hash\", StringType())\n",
    "])\n",
    "\n",
    "def get_market_prefixes(s3_client, bucket_name, base_prefix=\"USA/\"):\n",
    "    \"\"\"Get all market prefixes using bucket collection delimiter\"\"\"\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=base_prefix,\n",
    "        Delimiter=\"/\"\n",
    "    )\n",
    "    \n",
    "    # Extract just the market paths like \"USA/001F-COL-0CA2/\"\n",
    "    market_prefixes = [prefix['Prefix'].split('/')[1] for prefix in response.get('CommonPrefixes', [])]\n",
    "    return market_prefixes\n",
    "\n",
    "\n",
    "def transform_and_save_files(input_files, output_path, schema=schema, force_copy=True):\n",
    "    \"\"\"\n",
    "    Transform a list of Near Market data files and save to specified location\n",
    "    \n",
    "    Args:\n",
    "        input_files (list): List of S3 file paths to process\n",
    "        output_path (str): S3 path to save processed data\n",
    "        schema (StructType): Schema for the input data\n",
    "        force_copy (bool): Whether to overwrite existing data\n",
    "    \"\"\"\n",
    "\n",
    "    # Define H3 indexing functions\n",
    "    h3_udf = udf(lambda lat, lon: h3.geo_to_h3(lat, lon, H3_RESOLUTION_BASE), StringType())\n",
    "    h3_parent_udf = udf(lambda h3_index, res: h3.h3_to_parent(h3_index, res), StringType())\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No files provided to process\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Read all files into a Spark DataFrame\n",
    "        df = spark.read.csv(\n",
    "            input_files,\n",
    "            sep='\\t',\n",
    "            schema=schema,\n",
    "            inferSchema=False\n",
    "        )\n",
    "        \n",
    "        # Add H3 indices and other required columns\n",
    "        df = df.withColumn(\"h3i_15\", h3_udf(col(\"lat\"), col(\"lon\"))) \\\n",
    "            .withColumn(\"h3i_11\", h3_parent_udf(col(\"h3i_15\"), lit(11))) \\\n",
    "            .withColumn(\"h3i_9\", h3_parent_udf(col(\"h3i_15\"), lit(9))) \\\n",
    "            .withColumn(\"h3i_7\", h3_parent_udf(col(\"h3i_15\"), lit(7))) \\\n",
    "            .withColumn(\"h3i_5\", h3_parent_udf(col(\"h3i_15\"), lit(5))) \\\n",
    "            .withColumn(\"core_date\", from_unixtime('unix_ts').cast(DateType())) \\\n",
    "            .withColumn(\"timestamp\", to_timestamp(col(\"unix_ts\"))) \\\n",
    "            .withColumn(\"device_geo\", struct(col(\"lon\"), col(\"lat\"))) \\\n",
    "\n",
    "        \n",
    "        # Select and reorder columns to match Snowflake structure\n",
    "        df = df.select(\n",
    "            \"core_date\",\"device_id\",\"timestamp\",\n",
    "            \"unix_ts\",\"lat\",\"lon\",\n",
    "            \"device_geo\",\"datasource_id\",\n",
    "            \"accuracy\",\"device_ip\",\n",
    "            \"user_agent\",\"publisher_id\",\n",
    "            \"make\",\"os\",\"os_version\",\n",
    "            \"categories\",\"country_code\",\"market_hash\",\n",
    "            \"h3i_15\",\"h3i_11\",\"h3i_9\",\"h3i_7\",\"h3i_5\"\n",
    "        )\n",
    "        \n",
    "        # Write processed data\n",
    "        write_mode = \"overwrite\" if force_copy else \"append\"\n",
    "        df.coalesce(1).write.mode(write_mode) \\\n",
    "                .option(\"compression\", \"gzip\") \\\n",
    "                .parquet(output_path)\n",
    "        \n",
    "        # print(f\"Successfully processed {len(input_files)} files and saved to {output_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all market prefixes\n",
    "if MARKETS:\n",
    "    markets = MARKETS\n",
    "else:\n",
    "    markets = get_market_prefixes(S3_CLIENT, SOURCE_BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREFIX:\n",
    "    for market in tqdm(markets):  # Loop through markets\n",
    "        files = [f\"s3a://near-market-data/{market}/{PREFIX}part-{str(i).zfill(5)}.gz\" for i in range(50)]\n",
    "        output = f\"s3a://azira-backfill-snowflake/{market}/{PREFIX.replace('/', '_')}\"\n",
    "        success = transform_and_save_files(files, output)\n",
    "        print(files)\n",
    "    if success:\n",
    "        print(f\"Successfully processed and saved data to azira-backfill-snowflake\")\n",
    "    else:\n",
    "        print(\"Failed to process files.\")\n",
    "\n",
    "else:\n",
    "    for month in tqdm(range(1, 13)):  # Loop through months\n",
    "        for day in tqdm(range(1, 32)):  # Loop through days\n",
    "            for market in tqdm(markets):  # Loop through markets\n",
    "                formatted_day = str(day).zfill(2)\n",
    "                formatted_month = str(month).zfill(2)\n",
    "                current_prefix = f\"2024/{formatted_month}/{formatted_day}/\"\n",
    "                files = [f\"s3a://near-market-data/{market}/{current_prefix}part-{str(i).zfill(5)}.gz\" for i in range(50)]\n",
    "                output = f\"s3a://azira-backfill-snowflake/{market}/{current_prefix.replace('/', '_')}\"\n",
    "                success = transform_and_save_files(files, output)\n",
    "    if success:\n",
    "        print(f\"Successfully processed and saved data to azira-backfill-snowflake\")\n",
    "    else:\n",
    "        print(\"Failed to process files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
